# Datenwiederherstellungs-Anleitung

## Ãœbersicht

Diese Anleitung beschreibt, wie verlorene Address Datasets aus Activity Logs wiederhergestellt werden kÃ¶nnen.

## ğŸ”’ WICHTIG: Vor jedem Deployment/Push

**FÃ¼hre IMMER dieses Backup-Skript aus, bevor du Code-Ã„nderungen commitest und pushst:**

```bash
npx tsx backup-activity-logs.ts
```

Dieses Skript:
- âœ… Sammelt alle heutigen Activity Logs aus SQLite
- âœ… Sammelt alle heutigen Activity Logs aus Google Sheets
- âœ… Merged und dedupliziert die Logs
- âœ… LÃ¤dt das Backup nach Google Drive hoch
- âœ… Gibt dir eine BestÃ¤tigung mit Statistiken

**Nach erfolgreichem Backup kannst du sicher committen und pushen.**

---

## ğŸ“¥ Server-Datenbanken herunterladen

Falls du die lokalen SQLite-Datenbanken vom Server analysieren mÃ¶chtest:

```bash
# Railway CLI installieren (falls noch nicht installiert)
npm install -g @railway/cli

# Skript ausfÃ¼hrbar machen
chmod +x download-server-dbs.sh

# Datenbanken herunterladen
./download-server-dbs.sh
```

Dies erstellt ein Verzeichnis `downloaded-dbs-YYYY-MM-DD-HH-MM-SS/` mit allen SQLite-Datenbanken vom Server.

---

## ğŸ”§ Datenwiederherstellung aus Activity Logs

### Schritt 1: Backup-Datei besorgen

Lade die Backup-JSON-Datei von Google Drive herunter:

1. Gehe zu Google Drive â†’ Backup-Ordner
2. Suche nach `activity-logs-backup-YYYY-MM-DD-HH-MM-SS.json`
3. Lade die Datei herunter (z.B. nach `./data/temp/`)

### Schritt 2: Backup analysieren

```bash
npx tsx restore-address-datasets.ts ./data/temp/activity-logs-backup-2025-11-27.json
```

Dieses Skript:
- âœ… Analysiert alle Logs nach Address Dataset Operationen
- âœ… Extrahiert vollstÃ¤ndige Dataset-Informationen
- âœ… Exportiert die gefundenen Datasets als JSON zur manuellen ÃœberprÃ¼fung
- âœ… Zeigt Statistiken an

**Output:**
```
ğŸ“Š Datasets found: 42
ğŸ“„ Export file: ./data/temp/recovered-datasets-1732723456789.json
```

### Schritt 3: Exportierte Daten Ã¼berprÃ¼fen

Ã–ffne die exportierte JSON-Datei und Ã¼berprÃ¼fe die wiederhergestellten Datasets:

```bash
code ./data/temp/recovered-datasets-1732723456789.json
```

### Schritt 4: Daten wiederherstellen

Wenn die Daten korrekt aussehen, fÃ¼hre das Wiederherstellungs-Skript mit `--auto-restore` aus:

```bash
npx tsx restore-address-datasets.ts ./data/temp/activity-logs-backup-2025-11-27.json --auto-restore
```

Dies:
- âœ… PrÃ¼ft, welche Datasets bereits in Google Sheets existieren
- âœ… FÃ¼gt nur neue/fehlende Datasets hinzu
- âœ… Schreibt direkt nach Google Sheets (System Sheet â†’ "Adressen" tab)

**Nach dem Restore:**
- Die Datasets sind in Google Sheets
- Beim nÃ¤chsten Server-Neustart werden sie automatisch nach SQLite synchronisiert
- Der DatasetCache lÃ¤dt sie in den RAM

---

## ğŸ” Manuelle SQLite-Analyse

Falls du die SQLite-Datenbanken manuell analysieren mÃ¶chtest:

```bash
# Ã–ffne address-datasets.db
sqlite3 ./downloaded-dbs-*/address-datasets.db

# Zeige alle Datasets
SELECT * FROM address_datasets;

# Zeige Datasets fÃ¼r einen bestimmten User
SELECT * FROM address_datasets WHERE created_by = 'username';

# Zeige Datasets nach Datum
SELECT * FROM address_datasets ORDER BY created_at DESC;

# Exportiere als CSV
.mode csv
.output datasets-export.csv
SELECT * FROM address_datasets;
.quit
```

### Activity Logs analysieren

```bash
# Ã–ffne heutigen Log
sqlite3 ./downloaded-dbs-*/logs-2025-11-27.db

# Zeige alle Address Dataset Operationen
SELECT * FROM action_logs
WHERE endpoint = '/api/address-datasets'
ORDER BY timestamp DESC;

# Exportiere relevante Logs
.mode json
.output activity-logs-export.json
SELECT * FROM action_logs
WHERE endpoint LIKE '%address%'
ORDER BY timestamp DESC;
.quit
```

---

## ğŸ›¡ï¸ Bidirektionale Synchronisation

Das System synchronisiert automatisch bei jedem Server-Start:

1. **SQLite â†’ Sheets**: Lokale Daten, die nicht in Sheets sind, werden hochgeladen
2. **Sheets â†’ SQLite**: Sheets-Daten, die nicht lokal sind, werden heruntergeladen
3. **Konflikt-AuflÃ¶sung**: Bei Duplikaten wird die neuere Version (basierend auf `createdAt`) verwendet

**Verifizierung:**
- Server-Logs beim Start Ã¼berprÃ¼fen:
  ```
  [SystemSync] Phase 2: Bidirectional Sheets sync...
  [SystemSync]   addressDatasets: 42 synced (bidirectional)
  ```

---

## âœ… Dual-Write Verifikation

Das System schreibt **IMMER in beide Datenbanken**:

### createAddressDataset Flow:
1. **SQLite schreiben** (KRITISCH - wirft Error bei Fehler)
2. **RAM-Cache aktualisieren** (nur wenn SQLite erfolgreich)
3. **Google Sheets schreiben** (NON-BLOCKING - logged Warning bei Fehler)

### Beispiel-Code:
```typescript
// Step 1: Save to SQLite (PRIMARY - CRITICAL!)
try {
  addressDatasetsDB.upsert({...});
  console.log(`âœ… Saved to SQLite: ${id}`);
} catch (error) {
  console.error(`âŒ CRITICAL: Failed to save to SQLite, aborting:`, error);
  throw new Error(`Failed to persist dataset: ${error}`);
}

// Step 2: Add to RAM cache
datasetCache.addNew(fullDataset);

// Step 3: Save to Sheets (BACKUP - non-blocking)
try {
  await sheetsClient.spreadsheets.values.append({...});
  console.log(`âœ… Backed up to Sheets: ${id}`);
} catch (error) {
  console.warn(`âš ï¸ Failed to backup to Sheets (SQLite backup exists):`, error);
  datasetCache.set(fullDataset, true); // Mark as dirty for retry
}
```

**Wenn Sheets fehlschlÃ¤gt:**
- Dataset ist trotzdem in SQLite gesichert âœ…
- Dataset ist im RAM-Cache âœ…
- Dataset wird als "dirty" markiert âœ…
- Background-Sync versucht alle 60s, es nach Sheets zu schreiben âœ…

---

## ğŸ“Š Monitoring

### System DB Status Ã¼berprÃ¼fen

```bash
# Server-Logs anschauen
railway logs

# Nach Sync-Meldungen suchen
railway logs --filter "SystemSync"

# Rate Limit Status
railway logs --filter "Rate limit"
```

### Lokale Tests

```bash
# TypeScript kompilieren
npx tsc --noEmit

# Server lokal starten
npm run dev

# Logs beobachten
# Achte auf:
# - [SystemSync] Startup sync messages
# - [addressDatasets] Write operations
# - [BatchLogger] Flush operations
```

---

## ğŸ”„ Workflow-Zusammenfassung

### Vor Deployment:
1. âœ… `npx tsx backup-activity-logs.ts` ausfÃ¼hren
2. âœ… BestÃ¤tigung abwarten (Drive File ID, etc.)
3. âœ… Committen und pushen

### Nach Datenverlust:
1. âœ… Backup-Datei von Drive herunterladen
2. âœ… `npx tsx restore-address-datasets.ts <backup.json>` ausfÃ¼hren
3. âœ… Exportierte JSON Ã¼berprÃ¼fen
4. âœ… `npx tsx restore-address-datasets.ts <backup.json> --auto-restore` ausfÃ¼hren
5. âœ… Server neu starten â†’ Bidirektionale Sync lÃ¤uft automatisch

### Optional - Server DBs analysieren:
1. âœ… `./download-server-dbs.sh` ausfÃ¼hren
2. âœ… SQLite-Datenbanken mit `sqlite3` analysieren
3. âœ… Daten manuell exportieren/importieren falls nÃ¶tig

---

## ğŸ†˜ Troubleshooting

### "Google credentials not configured"
- ÃœberprÃ¼fe `.env`: `GOOGLE_APPLICATION_CREDENTIALS_JSON` oder `GOOGLE_SHEETS_KEY` gesetzt?
- ÃœberprÃ¼fe Railway: Environment Variables korrekt konfiguriert?

### "Backup file not found"
- Falscher Pfad? ÃœberprÃ¼fe den Dateinamen und Pfad
- Datei von Drive heruntergeladen? ÃœberprÃ¼fe Downloads-Ordner

### "No datasets found in backup"
- Activity Logs kÃ¶nnten leer sein (z.B. wenn heute keine Datasets erstellt wurden)
- Versuche ein Backup von gestern oder vorgestern

### "Rate limit error (429)"
- Warte 5 Minuten (globale Rate Limit Cooldown)
- Das System versucht automatisch erneut, wenn das Limit aufgehoben ist

### Server-DBs Download schlÃ¤gt fehl
- Railway CLI installiert? `npm install -g @railway/cli`
- Eingeloggt? `railway login`
- Richtiges Projekt? `railway link`

---

## ğŸ“§ Support

Bei Problemen:
1. ÃœberprÃ¼fe Server-Logs: `railway logs`
2. ÃœberprÃ¼fe lokale Logs: Konsolen-Output des Skripts
3. Kontaktiere den Entwickler mit:
   - Fehlermeldung
   - Verwendetes Kommando
   - Relevante Log-AuszÃ¼ge
